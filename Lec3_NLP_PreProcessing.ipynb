{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Text Preprocessing \n",
    "Written by : Ali Asadullah\\\n",
    "30 March 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Lowercasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"ID\": [1, 2, 3, 4, 5],\n",
    "    \"Noisy_Text\": [\n",
    "        \"<p>Hello <b>World!</b> This is an <a href='https://example.com'>example</a>.</p>\",\n",
    "        \"<div>Breaking News: <i>Big event happening now!</i> &#128512; </div>\",\n",
    "        \"<html><body><h1>Welcome to NLP</h1><p>Learn AI &amp; ML.</p></body></html>\",\n",
    "        \"<span style='color:red;'>Error! 404 <br> Page Not Found.</span>\",\n",
    "        \"Check this out: <script>alert('Hacked!');</script> Click <a href='spam.com'>here</a>!\"\n",
    "    ] \n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# df.to_csv(\"noisy_text_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Noisy_Text1'] = df['Noisy_Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Remove HTML Tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    cleaned_text = re.sub(r'<[^>]+>', '', text)\n",
    "    return cleaned_text\n",
    "df['Noisy_Text2'] = df['Noisy_Text'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile(r'<.*?>')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "df['Noisy_Text3'] = df['Noisy_Text'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Remove url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset with noisy text containing URLs\n",
    "data = {\n",
    "    \"ID\": [1, 2, 3, 4, 5],\n",
    "    \"Text\": [\n",
    "        \"Check out this amazing website: https://www.example.com!\",\n",
    "        \"Visit our blog at www.blogsite.com for more details.\",\n",
    "        \"Follow me on Twitter: http://twitter.com/user123\",\n",
    "        \"This is a normal sentence without any link.\",\n",
    "        \"Learn Python at https://python.org or http://docs.python.org!\"\n",
    "    ]\n",
    "}\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Display dataset\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub('', text)\n",
    "df['Clean_Text'] = df['Text'].apply(remove_url)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with noisy text containing punctuation\n",
    "data = {\n",
    "    \"ID\": [1, 2, 3, 4, 5],\n",
    "    \"Text\": [\n",
    "        \"Hello, world! NLP is amazing.\",\n",
    "        \"What's your name? My name is John.\",\n",
    "        \"Python (and AI) is powerful: it can do a lot!\",\n",
    "        \"Email me at abc@example.com - I'll reply ASAP!\",\n",
    "        \"Remove: commas, periods... and more!\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display dataset\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "df['Clean_Text1'] = df['Text'].apply(remove_punc)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))\n",
    "df['Clean_Text2'] = df['Text'].apply(remove_punc)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Chat Word Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words= {\"IMHO\":\"in my honest opinion \"}\n",
    "def chat_conversion(text):  \n",
    "    newtext = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            newtext.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            newtext.append(w)\n",
    "    return ' '.join(newtext)\n",
    "\n",
    "print(chat_conversion('IMHO , how are you?'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "text = \"This is a beutiful exmple of splling correcshun!\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "text = blob.correct()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Remove Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    newtext = []\n",
    "    for w in text.split():\n",
    "        if w.lower() in stopwords.words('english'):\n",
    "            newtext.append('')\n",
    "        else:\n",
    "            newtext.append(w)\n",
    "    return ' '.join(newtext)\n",
    "print(remove_stopwords(\"This is a beautiful example of spelling correction!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚úÖ \"This is a\" (stopwords) are removed.\n",
    "‚úÖ \"beautiful example spelling correction!\" remains.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Remove Emoji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(r\"[\"\n",
    "                                u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                                u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                                u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                                u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "                                u\"\\U00002702-\\U00002780\"  # Miscellaneous symbols\n",
    "                                u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "                               \"]+\", flags=re.UNICODE)  # Unicode flag\n",
    "    return emoji_pattern.sub(r'', text)  # Remove emoji from text\n",
    "\n",
    "# Example usage\n",
    "text_with_emoji = \"Hello üòäüåçüöÄ!\"\n",
    "print(remove_emoji(text_with_emoji))  # Output: \"Hello !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "emoji.demojize(\"Python is üòä and very üåç and extreme üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use Split Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenization\n",
    "sentence = \"Today is first Day of Eid-ul-Fitr and I am very Happy today \"\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenization\n",
    "sentence = \"I am happy today.Because I am going to meet my friend.Today was my life golden day\"\n",
    "sentence.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = \"Today is first Day of Eid-ul-Fitr\"\n",
    "tokens = re.findall(\"[\\w']+\", sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"lorem5 ipsum dolor sit amet, consectetur adipiscing elit! Sed do eiusmod tempor incididunt ut! labore et dolore magna aliqua! Ut enim ad minim veniam, quis nostrud? exercitation ullamco laboris nisi! ut aliquip ex ea commodo consequat? Duis aute irure dolor in reprehenderit? in voluptate velit esse cillum dolore eu? fugiat nulla pariatur! Excepteur sint occaecat !cupidatat non proident, sunt in culpa qui officia? deserunt mollit anim id est laborum.\"\n",
    "tokens = re.compile(\"[.?!]+\").split(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 . NLTK Library has 2 functions to create token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentence = \"Today is first Day of Eid-ul-Fitr  \"\n",
    "word_tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 . Spacy Library converts doc to tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Today is first Day of Eid-ul-Fitr  \")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.Stemming (Convert Number to its root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "stem_words(\"walk walked walked walking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Word Lemmatization using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python script uses the NLTK (Natural Language Toolkit) library to perform lemmatization,\n",
    "\n",
    "which is the process of reducing words to their base or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"He was running and eating at the same time. He has a bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# Define punctuation marks to remove\n",
    "punctuations = \"?:!.,\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Remove punctuation using list comprehension (fixed issue)\n",
    "sentence_words = [word for word in sentence_words if word not in punctuations]\n",
    "\n",
    "# Print header\n",
    "print(\"{:<20} {:<20}\".format(\"Word\", \"Lemma\"))\n",
    "\n",
    "# Lemmatize each word correctly (using pos='v' for verbs)\n",
    "for word in sentence_words:\n",
    "    lemma = wordnet_lemmatizer.lemmatize(word, pos='v')  # Use verb lemmatization\n",
    "    print(\"{:<20} {:<20}\".format(word, lemma))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
